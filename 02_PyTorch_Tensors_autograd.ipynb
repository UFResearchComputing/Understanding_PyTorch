{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UF Research Computing  \n",
    "\n",
    "![UF Research Computing Logo](images/ufrc_logo.png)\n",
    "\n",
    "\n",
    "This tutorial is adapted from: [Understanding PyTorch with an example: a step-by-step tutorial](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e) by Daniel Godoy.\n",
    "\n",
    "# Understanding PyTorch: Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "First, we need to cover a **few basic concepts** that may throw you off-balance if you don’t grasp them well enough before going full-force on modeling.\n",
    "\n",
    "In Deep Learning, we see **tensors** everywhere. Well, Google’s framework is called TensorFlow for a reason! *What is a tensor, anyway?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "In Numpy, you may have an **array** that has **three dimensions**, right? That is, technically speaking, a **tensor**.\n",
    "\n",
    "A **scalar** (a single number) has **zero** dimensions, a **vector** has **one** dimension, a **matrix** has **two** dimensions and a **tensor** has **three or more** dimensions. That’s it!\n",
    "\n",
    "But, to keep things simple, it is commonplace to call vectors and matrices tensors as well — so, from now on, **everything is either a scalar or a tensor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data, Devices and CUDA\n",
    "\n",
    "*\"How do we go from Numpy’s arrays to PyTorch’s tensors\"*, you ask? That’s what [`from_numpy()`](https://pytorch.org/docs/stable/torch.html#torch.from_numpy) is good for. It returns a **CPU tensor**, though.\n",
    "\n",
    "*\"But I want to use my fancy GPU…\"*, you say. No worries, that’s what [`to()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to) is good for. It sends your tensor to whatever **device** you specify, including your **GPU** (referred to as `cuda` or `cuda:0`).\n",
    "\n",
    "*\"What if I want my code to fallback to CPU if no GPU is available?\"*, you may be wondering… PyTorch got your back once more — you can use [`cuda.is_available()`](https://pytorch.org/docs/stable/cuda.html?highlight=is_available#torch.cuda.is_available) to find out if you have a GPU at your disposal and set your device accordingly.\n",
    "\n",
    "You can also easily **cast** it to a lower precision (32-bit float) using [`float()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.float)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to regenerate the data we used in the previous notebook. The code for that is in the script [`generate_data.py`](generate_data.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run generate_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42) # Comment out for random results.\n",
    "x = np.random.rand(100, 1)\n",
    "\n",
    "# y = 1 + 2x + Gaussian noise\n",
    "y = 1 + 2 * x + .1 * np.random.randn(100, 1)\n",
    "\n",
    "\n",
    "# Shuffles the indices to split train and validation datasets\n",
    "idx = np.arange(100)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:80]\n",
    "\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[80:]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Type of x_train: <class 'numpy.ndarray'>\n",
      "Type of x_train_tensor: <class 'torch.Tensor'>\n",
      "Type of x_train_tensor with .type: torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "# and then we send them to the chosen device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "# Here we can see the difference - notice that .type() is more useful\n",
    "# since it also tells us WHERE the tensor is (device)\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Type of x_train: {type(x_train)}\")\n",
    "print(f\"Type of x_train_tensor: {type(x_train_tensor)}\")\n",
    "print(f\"Type of x_train_tensor with .type: {x_train_tensor.type()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the types of both variables, you’ll get what you’d expect: `numpy.ndarray` for the first one and `torch.Tensor` for the second one.\n",
    "\n",
    "But where does your nice tensor \"live\"? In your CPU or your GPU? You can’t say… but if you use PyTorch’s `type()`, it will reveal its location — `torch.cuda.FloatTensor` — a GPU tensor in this case.\n",
    "\n",
    "We can also go the other way around, turning tensors back into Numpy arrays, using [`numpy()`](https://pytorch.org/docs/stable/tensors.html?highlight=numpy#torch.Tensor.numpy). It should be easy as `x_train_tensor.numpy()` but…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-df6947de457e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "z = x_train_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Numpy cannot handle GPU tensors… you need to make them CPU tensors first using cpu()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x_train_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Parameters\n",
    "\n",
    "What distinguishes a *tensor* used for *data* — like the ones we’ve just created — from a *tensor* used as a (*trainable*) *parameter/weight*?\n",
    "\n",
    "The latter tensors require the **computation of its gradients**, so we can **update** their values (the parameters' values, that is). That’s what the `requires_grad=True` argument is good for. It tells PyTorch we want it to compute gradients for us.\n",
    "\n",
    "You may be tempted to create a simple tensor for a parameter and, later on, send it to your chosen device, as we did with our data, right? Not so fast…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367], requires_grad=True)\n",
      "tensor([0.1288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# FIRST\n",
    "# Initializes parameters \"a\" and \"b\" randomly, ALMOST as we did in Numpy\n",
    "# since we want to apply gradient descent on these parameters, we need\n",
    "# to set REQUIRES_GRAD = TRUE\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first chunk of code creates two nice tensors for our parameters, gradients and all. But they are CPU tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2345], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([0.2303], device='cuda:0', grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "# SECOND\n",
    "# But what if we want to run it on a GPU? We could just send them to device, right?\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "# Sorry, but NO! The to(device) \"shadows\" the gradient.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second chunk of code, we tried the **naive** approach of sending them to our GPU. We succeeded in sending them to another device, but we \"**lost**\" the **gradients** somehow…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1229], device='cuda:0', requires_grad=True)\n",
      "tensor([-0.1863], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# THIRD\n",
    "# We can either create regular tensors and send them to the device (as we did with our data)\n",
    "a = torch.randn(1, dtype=torch.float).to(device)\n",
    "b = torch.randn(1, dtype=torch.float).to(device)\n",
    "# and THEN set them as requiring gradients...\n",
    "a.requires_grad_()\n",
    "b.requires_grad_()\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third chunk, we **first** send our tensors to the **device** and **then** use [`requires_grad_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.requires_grad_) method to set its `requires_grad` to `True` in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, every method that **ends** with an **underscore (_)** makes changes **in-place**, meaning, they will **modify** the underlying variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the last approach worked fine, it is much better to **assign** tensors to a **device** at the moment of their **creation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1940], device='cuda:0', requires_grad=True)\n",
      "tensor([0.1391], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# We can specify the device at the moment of creation - RECOMMENDED!\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much easier, right?\n",
    "\n",
    "Now that we know how to create tensors that require gradients, let’s see how PyTorch handles them — that’s the role of the…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Autograd is PyTorch’s *automatic differentiation package*. Thanks to it, we **don’t need to worry** about *partial derivatives*, *chain rule* or anything like it.\n",
    "\n",
    "So, how do we tell PyTorch to do its thing and **compute all gradients**? That’s what [`backward()`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward) is good for.\n",
    "\n",
    "Do you remember the **starting point** for **computing the gradients**? It was the **loss**, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the `backward()` method from the corresponding Python variable, like, `loss.backward()`.\n",
    "\n",
    "What about the **actual values** of the **gradients**? We can inspect them by looking at the [`grad`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.grad) **attribute** of a tensor.\n",
    "\n",
    "If you check the method’s documentation, it clearly states that **gradients are accumulated**. So, every time we use the **gradients** to **update** the parameters, we need to **zero the gradients afterwards**. And that’s what [`zero_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.zero_) is good for.\n",
    "\n",
    "What does the **underscore (_)** at the end of the method name mean? Do you remember? If not, scroll back to the previous section and find out.\n",
    "\n",
    "So, let’s **ditch** the **manual computation of gradients** and use both `backward()` and `zero_()` methods instead.\n",
    "\n",
    "That’s it? Well, pretty much… but, **there is always a catch**, and this time it has to do with the **update** of the **parameters**…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad is: tensor([-3.3881], device='cuda:0')\n",
      "b.grad is: tensor([-1.9439], device='cuda:0')\n",
      "a is: tensor([0.5328], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "a.grad is: None\n",
      "b.grad is: None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1bfb1de4464a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# FIRST ATTEMPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# AttributeError: 'NoneType' object has no attribute 'zero_'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"a is: {a}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # No more manual computation of gradients! \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    \n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward()\n",
    "    # Let's check the computed gradients...\n",
    "    print(f\"a.grad is: {a.grad}\")\n",
    "    print(f\"b.grad is: {b.grad}\")\n",
    "    \n",
    "    # What about UPDATING the parameters? Not so fast...\n",
    "    \n",
    "    # FIRST ATTEMPT\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    a = a - lr * a.grad\n",
    "    b = b - lr * b.grad\n",
    "    print(f\"a is: {a}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first attempt, if we use the same update structure as in our Numpy code, we’ll get the weird error.\n",
    "\n",
    "But we can get a hint of what’s going on by looking at the tensor itself — once again we \"**lost**\" the **gradient** while reassigning the update results to our parameters. Thus, the `grad` attribute turns out to be None and it raises the error…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-03d3e395f9f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SECOND ATTEMPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# SECOND ATTEMPT\n",
    "# RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
    "a -= lr * a.grad\n",
    "b -= lr * b.grad        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then change it slightly, using a familiar **in-place Python assignment** in our second attempt. And, once again, PyTorch complains about it and raises an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Why?!* It turns out to be a case of \"**too much of a good thing**\". The culprit is PyTorch's ability to build a **dynamic computation graph** from *every* **Python operation** that involves any **gradient-computing tensor** or **its dependencies**.\n",
    "\n",
    "> We'll go deeper into the inner workings of the dynamic computation graph in the next section.\n",
    "\n",
    "So, how do we tell PyTorch to \"**back off**\" and let us **update our parameters** without messing up with its *fancy dynamic computation graph*? That's what [`torch.no_grad()`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad) is good for. It allows us to **perform regular Python operations on tensors, independent of PyTorch’s computation graph.**\n",
    "\n",
    "Finally, we managed to successfully run our model and get the **resulting parameters**. Surely enough, they **match** the ones we got in our Numpy-only implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "   a.grad is: tensor([-3.3881], device='cuda:0')\n",
      "   b.grad is: tensor([-1.9439], device='cuda:0')\n",
      "Epoch: 100\n",
      "   a.grad is: tensor([0.0188], device='cuda:0')\n",
      "   b.grad is: tensor([-0.0367], device='cuda:0')\n",
      "Epoch: 200\n",
      "   a.grad is: tensor([0.0041], device='cuda:0')\n",
      "   b.grad is: tensor([-0.0080], device='cuda:0')\n",
      "Epoch: 300\n",
      "   a.grad is: tensor([0.0009], device='cuda:0')\n",
      "   b.grad is: tensor([-0.0018], device='cuda:0')\n",
      "Epoch: 400\n",
      "   a.grad is: tensor([0.0002], device='cuda:0')\n",
      "   b.grad is: tensor([-0.0004], device='cuda:0')\n",
      "Epoch: 500\n",
      "   a.grad is: tensor([4.2574e-05], device='cuda:0')\n",
      "   b.grad is: tensor([-8.3295e-05], device='cuda:0')\n",
      "Epoch: 600\n",
      "   a.grad is: tensor([9.3249e-06], device='cuda:0')\n",
      "   b.grad is: tensor([-1.8163e-05], device='cuda:0')\n",
      "Epoch: 700\n",
      "   a.grad is: tensor([1.9097e-06], device='cuda:0')\n",
      "   b.grad is: tensor([-4.1103e-06], device='cuda:0')\n",
      "Epoch: 800\n",
      "   a.grad is: tensor([5.1083e-07], device='cuda:0')\n",
      "   b.grad is: tensor([-8.8313e-07], device='cuda:0')\n",
      "Epoch: 900\n",
      "   a.grad is: tensor([5.4762e-07], device='cuda:0')\n",
      "   b.grad is: tensor([-5.7358e-07], device='cuda:0')\n",
      "\n",
      "a is: tensor([1.0235], device='cuda:0', requires_grad=True)\n",
      "b is: tensor([1.9690], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # No more manual computation of gradients! \n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    \n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward()\n",
    "    # Let's check the computed gradients...\n",
    "    if epoch % 100 == 0:    # print every 100 epochs\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"   a.grad is: {a.grad}\")\n",
    "        print(f\"   b.grad is: {b.grad}\")\n",
    "    \n",
    "     # THIRD ATTEMPT\n",
    "    # We need to use NO_GRAD to keep the update out of the gradient computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(f\"\\na is: {a}\")\n",
    "print(f\"b is: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Continue to part 3 of the tutorial: 03_DynamicComputationGraph.ipynb](03_DynamicComputationGraph.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.1",
   "language": "python",
   "name": "tensorflow-2.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
